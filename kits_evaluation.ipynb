{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h1>Evaluation</h3>\n",
    "Here we will Evaluate the results of teh segmentation on three metrics: Dice-Score, HD95-Score, ASSD-Score. HD95 and ASSD depend on the target spacing which is why we have to specify the traget spacing of the model we want to evaluate"
   ],
   "id": "763afa6a5763e09e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "target_spacing=(1.0, 1.0, 1.0)",
   "id": "4c16be9a247304a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Native Spacing</h3>\n",
    "First we evaluate the model on the images with the native resolution"
   ],
   "id": "4b25b8ec5c681a42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from panoptica import Panoptica_Statistic, InputType, Panoptica_Evaluator,Panoptica_Aggregator, ConnectedComponentsInstanceApproximator, NaiveThresholdMatching\n",
    "from panoptica.metrics import Metric\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import sys\n",
    "# Add script directory to Python path\n",
    "module_path = r\"KiTS23/scripts\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import resample_to_target\n",
    "importlib.reload(resample_to_target)\n",
    "\n",
    "from resample_to_target import resample_dataset"
   ],
   "id": "5166fae1d6f488bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Enter the path to your predictions\n",
    "pred_dir = \"KiTS23/predictions/original_resolution\"\n",
    "\n",
    "# Enter the path to your ground truths\n",
    "gt_dir = \"KiTS23/dataset/nnUNet_data/test/nnUNet_raw/Dataset220_KiTS2023/labelsTr\"\n",
    "\n",
    "output_file = \"KiTS23/evaluation/evaluation_native.tsv\"\n",
    "\n",
    "# Enter the voxel spacing of your model (the one that was used to get the predictions)\n",
    "voxel_spacing = (1.0, 1.0, 1.0)"
   ],
   "id": "6a8b1b190b303477",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Collect all case names from the ground truth folder ===\n",
    "case_ids = sorted([\n",
    "    f.replace(\".nii.gz\", \"\")\n",
    "    for f in os.listdir(gt_dir)\n",
    "    if f.endswith(\".nii.gz\")\n",
    "])\n",
    "\n",
    "# === Create PAIR ===\n",
    "PAIR = []\n",
    "\n",
    "for case_id in case_ids:\n",
    "    pred_path = os.path.join(pred_dir, case_id + \".nii.gz\")\n",
    "    gt_path = os.path.join(gt_dir, case_id + \".nii.gz\")\n",
    "\n",
    "    if not os.path.exists(pred_path):\n",
    "        print(f\"[Warning] Prediction for {case_id} not found, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load prediction and GT\n",
    "    pred_img = nib.load(pred_path)\n",
    "    gt_img = nib.load(gt_path)\n",
    "\n",
    "    pred = pred_img.get_fdata().astype(np.uint8)\n",
    "    mask = gt_img.get_fdata().astype(np.uint8)\n",
    "\n",
    "    # Optional: check shape match\n",
    "    if pred.shape != mask.shape:\n",
    "        print(f\"[Error] Shape mismatch in {case_id}: pred {pred.shape}, gt {mask.shape}\")\n",
    "        continue\n",
    "\n",
    "    PAIR.append((pred, mask, case_id))\n",
    "\n",
    "print(f\"Loaded {len(PAIR)} pairs for evaluation.\")\n",
    "\n",
    "evaluator = Panoptica_Aggregator(\n",
    "    Panoptica_Evaluator.load_from_config(\"KiTS23/scripts/panoptica_evaluator_kits23.yaml\"),\n",
    "    output_file = output_file,\n",
    "    log_times = True,\n",
    "    continue_file = True,\n",
    ")\n",
    "\n",
    "for pred, gt, case in PAIR:\n",
    "    evaluator.evaluate(pred, gt, case, voxelspacing=voxel_spacing)"
   ],
   "id": "440cff1bde34b192",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Target Spacing</h3>\n",
    "Now we evaluate the model on the images with the target resolution. We have to resample the ground thruths to the target resolution aswell."
   ],
   "id": "63ab16cf39243e28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add script directory to Python path\n",
    "module_path = r\"KiTS23/scripts\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Define input folder and target spacing\n",
    "input_folder = \"KiTS23/dataset/nnUNet_data/test/nnUNet_raw/Dataset220_KiTS2023/labelsTr\"\n",
    "target_spacing = (2.0, 2.0, 2.0)\n",
    "\n",
    "# Call the function\n",
    "resample_dataset(\n",
    "    input_folder=input_folder,\n",
    "    target_spacing=target_spacing,\n",
    "    seg=True\n",
    ")"
   ],
   "id": "381b6b4339428b35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we do the evaluation on the target resolution",
   "id": "50f5bd221c90ecc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Enter the path to your predictions\n",
    "pred_dir = \"KiTS23/predictions/target_resolution\"\n",
    "\n",
    "# Enter the path to your ground truths\n",
    "gt_dir = \"KiTS23/dataset/nnUNet_data/test/nnUNet_raw_resampled/Dataset220_KiTS2023/labelsTr\"\n",
    "\n",
    "output_file = \"KiTS23/evaluation/evaluation_tsv/evaluation_target.tsv\"\n",
    "\n",
    "# Enter the voxel spacing of your model (the one that was used to get the predictions)\n",
    "voxel_spacing = (2.0, 2.0, 2.0)"
   ],
   "id": "fe298c7a23d4f7b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Collect all case names from the ground truth folder ===\n",
    "case_ids = sorted([\n",
    "    f.replace(\".nii.gz\", \"\")\n",
    "    for f in os.listdir(gt_dir)\n",
    "    if f.endswith(\".nii.gz\")\n",
    "])\n",
    "\n",
    "# === Create PAIR ===\n",
    "PAIR = []\n",
    "\n",
    "for case_id in case_ids:\n",
    "    pred_path = os.path.join(pred_dir, case_id + \".nii.gz\")\n",
    "    gt_path = os.path.join(gt_dir, case_id + \".nii.gz\")\n",
    "\n",
    "    if not os.path.exists(pred_path):\n",
    "        print(f\"[Warning] Prediction for {case_id} not found, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load prediction and GT\n",
    "    pred_img = nib.load(pred_path)\n",
    "    gt_img = nib.load(gt_path)\n",
    "\n",
    "    pred = pred_img.get_fdata().astype(np.uint8)\n",
    "    mask = gt_img.get_fdata().astype(np.uint8)\n",
    "\n",
    "    # Optional: check shape match\n",
    "    if pred.shape != mask.shape:\n",
    "        print(f\"[Error] Shape mismatch in {case_id}: pred {pred.shape}, gt {mask.shape}\")\n",
    "        continue\n",
    "\n",
    "    PAIR.append((pred, mask, case_id))\n",
    "\n",
    "print(f\"Loaded {len(PAIR)} pairs for evaluation.\")\n",
    "\n",
    "evaluator = Panoptica_Aggregator(\n",
    "    Panoptica_Evaluator.load_from_config(\"KiTS23/scripts/panoptica_evaluator_kits23.yaml\"),\n",
    "    output_file = output_file,\n",
    "    log_times = True,\n",
    "    continue_file = True,\n",
    ")\n",
    "\n",
    "for pred, gt, case in PAIR:\n",
    "    evaluator.evaluate(pred, gt, case, voxelspacing=voxel_spacing)\n"
   ],
   "id": "e952f2df12aab811",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Plots</h3>\n",
    "Now we can create plots with the tsv files we just created"
   ],
   "id": "c0a39fdb9b186048"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from panoptica import Panoptica_Statistic\n",
    "from panoptica_statistics import make_curve_over_setups\n",
    "\n",
    "# Add additional tsv files here to compare models\n",
    "tsv1 = \"KiTS23/evaluation/evaluation_tsv/example.tsv\"\n",
    "\n",
    "stat1 = Panoptica_Statistic.from_file(tsv1)\n",
    "\n",
    "# Also add new models in this dict\n",
    "statistics_dict = {\n",
    "    f\"Model_Name\": stat1,\n",
    "}\n",
    "\n",
    "metric = \"global_bin_dsc\"  # or any metric name found in your TSVs\n",
    "groups = [\"kidney\", \"masses\", \"tumor\"]  # optional: choose groups to compare\n",
    "\n",
    "fig = make_curve_over_setups(\n",
    "    statistics_dict=statistics_dict,\n",
    "    metric=metric,\n",
    "    groups=groups,\n",
    "    plot_as_barchart=True,\n",
    "    plot_std=True,\n",
    "    figure_title=None,\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Dice\",\n",
    "    height=600,\n",
    "    width=1200,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"KiTS23/evaluation/plots/compare_models.png\")"
   ],
   "id": "81583252ad716d72",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
